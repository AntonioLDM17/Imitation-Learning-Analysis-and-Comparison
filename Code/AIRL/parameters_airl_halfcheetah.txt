--env: halfcheetah
--timesteps: 2000000
--seed: 44
--demo-batch-size: 2048
--demo_episodes 100
# SB3 PPO generator with TensorBoard logging
learner = PPO(
        "MlpPolicy",
        env,
        batch_size=128, # Original 64
        ent_coef=0.01,
        learning_rate=1e-3, # Original 5e-4
        gamma=0.95,
        clip_range=0.1,
        vf_coef=0.1,
        n_epochs=5,
        seed=SEED,
        verbose=1,
        tensorboard_log=LOG_DIR,
)
# Reward network for AIRL
reward_net = BasicShapedRewardNet(
    observation_space=env.observation_space,
    action_space=env.action_space,
    reward_hid_sizes=(128, 128),
    potential_hid_sizes=(128, 128, 64),
    use_state=True,
    use_action=True,
    use_next_state=True,
    use_done=False,
    discount_factor=0.99,
    normalize_input_layer=RunningNorm,
)

# Instantiate AIRL trainer with HierarchicalLogger
airl_trainer = AIRL(
    demonstrations=demonstrations,
    demo_batch_size=args.demo_batch_size,
    gen_replay_buffer_capacity=1024,
    n_disc_updates_per_round=24, 
    venv=env,
    gen_algo=learner,
    reward_net=reward_net,
    allow_variable_horizon=True,
    init_tensorboard=True,
    init_tensorboard_graph=False,
    custom_logger=il_logger,
)

