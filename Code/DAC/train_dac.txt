import os
import sys
import types
import argparse
import numpy as np
import gymnasium as gym
from imitation.util.util import make_vec_env
from imitation.data.wrappers import RolloutInfoWrapper

import torch
import torch.nn.functional as F
import torch.optim as optim
from torch.distributions import Categorical

# Create dummy modules for "mujoco_py" (to avoid compiling its extensions)
dummy = types.ModuleType("mujoco_py")
dummy.builder = types.ModuleType("mujoco_py.builder")
dummy.locomotion = types.ModuleType("mujoco_py.locomotion")
sys.modules["mujoco_py"] = dummy
sys.modules["mujoco_py.builder"] = dummy.builder
sys.modules["mujoco_py.locomotion"] = dummy.locomotion

# Import everything from dac.py (with BCEWithLogitsLoss for the discriminator)
from dac import (
    DAC,
    DAC_Discriminator,
    DiscreteActor,
    DiscreteCritic,
    ContinuousActor,
    ContinuousCritic
)

# For logging metrics into TensorBoard logs readable by TensorFlow
from torch.utils.tensorboard import SummaryWriter


def flatten_demonstrations(demos):
    """
    Flattens a list of trajectories (TrajectoryWithRew objects)
    into a list of transitions of the form: (obs, act, rew, next_obs, done).
    """
    transitions = []
    for traj in demos:
        N = len(traj.acts)
        for i in range(N):
            obs_i = traj.obs[i]
            act_i = traj.acts[i]
            rew_i = traj.rews[i]
            next_obs = traj.obs[i + 1]
            done_flag = 1.0 if i == N - 1 else 0.0
            transitions.append((obs_i, act_i, rew_i, next_obs, done_flag))
    return transitions


def main():
    parser = argparse.ArgumentParser(description="Train a DAC model (Discriminator-Actor-Critic).")
    parser.add_argument("--env", type=str, choices=["cartpole", "halfcheetah"], default="cartpole",
                        help="Environment: 'cartpole' (discrete) or 'halfcheetah' (continuous).")
    parser.add_argument("--timesteps", type=int, default=200000,
                        help="Total timesteps for training.")
    parser.add_argument("--seed", type=int, default=42, help="Random seed.")
    args = parser.parse_args()

    # Map the env argument
    if args.env == "cartpole":
        ENV_NAME = "CartPole-v1"
    elif args.env == "halfcheetah":
        ENV_NAME = "HalfCheetah-v4"
    else:
        raise ValueError("Invalid environment option.")

    TOTAL_TIMESTEPS = args.timesteps
    SEED = args.seed

    # Prepare directories
    DEMO_DIR = "demonstrations"
    DEMO_FILENAME = f"{args.env}_demonstrations.npy"
    MODELS_DIR = "models"
    MODEL_NAME = f"dac_{args.env}"
    LOG_DIR = f"logs/dac_{args.env}"
    os.makedirs(MODELS_DIR, exist_ok=True)
    os.makedirs(LOG_DIR, exist_ok=True)

    # Make the environment (vectorized with 1 env)
    env = make_vec_env(
        ENV_NAME,
        rng=np.random.default_rng(SEED),
        n_envs=1,
        post_wrappers=[lambda e, _: RolloutInfoWrapper(e)]
    )

    # Load expert demonstrations
    demo_path = os.path.join(DEMO_DIR, DEMO_FILENAME)
    demonstrations = np.load(demo_path, allow_pickle=True)
    if isinstance(demonstrations, np.ndarray):
        demonstrations = demonstrations.tolist()

    # Flatten if needed
    try:
        _ = iter(demonstrations[0])  # if this fails, we flatten
    except TypeError:
        demonstrations = flatten_demonstrations(demonstrations)

    # Check if discrete or continuous
    if isinstance(env.action_space, gym.spaces.Discrete):
        is_discrete = True
        n_actions = env.action_space.n
        act_dim = n_actions
    else:
        is_discrete = False
        act_dim = env.action_space.shape[0]

    obs_dim = env.observation_space.shape[0]
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # Initialize actor, critic
    if is_discrete:
        actor = DiscreteActor(obs_dim, n_actions).to(device)
        critic = DiscreteCritic(obs_dim, n_actions).to(device)
        disc_act_dim = n_actions
    else:
        actor = ContinuousActor(obs_dim, act_dim).to(device)
        critic = ContinuousCritic(obs_dim, act_dim).to(device)
        disc_act_dim = act_dim

    # Updated DAC_Discriminator with raw logits & BCEWithLogitsLoss
    discriminator = DAC_Discriminator(obs_dim=obs_dim, act_dim=disc_act_dim).to(device)

    # Lower LR, weight decay for disc, plus some tune for actor/critic
    actor_optimizer = optim.Adam(actor.parameters(), lr=4e-5)
    critic_optimizer = optim.Adam(critic.parameters(), lr=6e-5)
    disc_optimizer = optim.Adam(
        discriminator.parameters(),
        lr=5e-5,
        weight_decay=1e-5
    )

    # Create the DAC trainer
    # We'll do label smoothing in update_discriminator by editing dac.py,
    # but if you want to do it here, see notes below.
    dac_trainer = DAC(
        actor=actor,
        critic=critic,
        discriminator=discriminator,
        actor_optimizer=actor_optimizer,
        critic_optimizer=critic_optimizer,
        disc_optimizer=disc_optimizer,
        device=device,
        is_discrete=is_discrete,
        n_actions=n_actions if is_discrete else None
    )

    print("Starting DAC training...")

    # Initialize TensorBoard writer
    writer = SummaryWriter(log_dir=LOG_DIR)

    # Reset env; extract single obs
    raw_obs = env.reset()[0]
    if isinstance(raw_obs, np.ndarray) and raw_obs.ndim == 2:
        obs = raw_obs[0]
    else:
        obs = raw_obs

    if obs.shape[0] != obs_dim:
        raise ValueError(f"Observation shape mismatch: expected {obs_dim}, got {obs.shape[0]}")

    # We'll track reward sums for episodes completed between logs
    episodes_since_log = 0
    reward_sum_since_log = 0.0
    episode_reward = 0.0
    timestep = 0

    # Logging/printing interval (in timesteps)
    PRINT_INTERVAL = 1000

    while timestep < TOTAL_TIMESTEPS:
        # Turn obs into a batch of size 1
        obs_tensor = torch.tensor(obs, dtype=torch.float32, device=device).unsqueeze(0)
        with torch.no_grad():
            if is_discrete:
                # Sample an integer action from the logits
                logits = actor(obs_tensor)
                dist = Categorical(logits=logits)
                action = int(dist.sample().item())
            else:
                # Continuous actor outputs a vector
                action = actor(obs_tensor).cpu().numpy()[0]

        # Step env with a single action
        if is_discrete:
            action_batch = np.array([action])  # shape (1,)
        else:
            action_batch = np.array([action])  # shape (1, act_dim)

        result = env.step(action_batch)
        if len(result) == 5:
            next_obs, reward_batch, terminated, truncated, _ = result
            done = terminated[0] or truncated[0]
        elif len(result) == 4:
            next_obs, reward_batch, done, info = result
            if isinstance(done, (list, np.ndarray)):
                done = done[0]
        else:
            raise ValueError("Unexpected return format from env.step()")

        next_obs = next_obs[0]
        reward = reward_batch[0]

        # Store transition
        dac_trainer.replay_buffer.append((obs, action, reward, next_obs, float(done)))

        obs = next_obs
        episode_reward += reward
        timestep += 1

        # Handle episode end
        if done:
            episodes_since_log += 1
            reward_sum_since_log += episode_reward
            raw_obs = env.reset()[0]
            if isinstance(raw_obs, np.ndarray) and raw_obs.ndim == 2:
                obs = raw_obs[0]
            else:
                obs = raw_obs
            episode_reward = 0.0

        # Once enough samples are in the replay buffer, do updates
        if len(dac_trainer.replay_buffer) >= 64:
            indices = np.random.choice(len(dac_trainer.replay_buffer), 64, replace=False)
            batch = [dac_trainer.replay_buffer[i] for i in indices]
            obs_b, act_b, reward_b, next_obs_b, done_b = zip(*batch)

            obs_b = torch.tensor(np.array(obs_b), dtype=torch.float32)
            if is_discrete:
                act_b = torch.tensor(np.array(act_b), dtype=torch.long).squeeze(-1)
            else:
                act_b = torch.tensor(np.array(act_b), dtype=torch.float32)
            reward_b = torch.tensor(np.array(reward_b), dtype=torch.float32).unsqueeze(1)
            next_obs_b = torch.tensor(np.array(next_obs_b), dtype=torch.float32)
            done_b = torch.tensor(np.array(done_b), dtype=torch.float32).unsqueeze(1)

            actor_loss, critic_loss = dac_trainer.update_actor_critic(
                (obs_b, act_b, reward_b, next_obs_b, done_b)
            )

            # Update Discriminator fewer times (2 instead of 5)
            disc_loss = 0.0
            for _ in range(2):
                expert_indices = np.random.choice(len(demonstrations), 64, replace=True)
                expert_batch = [demonstrations[i] for i in expert_indices]
                expert_obs, expert_act, _, _, _ = zip(*expert_batch)

                expert_obs = torch.tensor(np.array(expert_obs), dtype=torch.float32)
                if is_discrete:
                    # Convert to integer, then do one-hot before passing to disc
                    expert_act = torch.tensor(np.array(expert_act), dtype=torch.long).squeeze(-1)
                    expert_act = F.one_hot(expert_act, num_classes=n_actions).float()
                else:
                    expert_act = torch.tensor(np.array(expert_act), dtype=torch.float32)

                policy_indices = np.random.choice(len(dac_trainer.replay_buffer), 64, replace=True)
                policy_batch = [dac_trainer.replay_buffer[i] for i in policy_indices]
                policy_obs, policy_act, _, _, _ = zip(*policy_batch)

                policy_obs = torch.tensor(np.array(policy_obs), dtype=torch.float32)
                if is_discrete:
                    policy_act = torch.tensor(np.array(policy_act), dtype=torch.long).squeeze(-1)
                    policy_act = F.one_hot(policy_act, num_classes=n_actions).float()
                else:
                    policy_act = torch.tensor(np.array(policy_act), dtype=torch.float32)

                # This call does label smoothing inside DAC (see below)
                disc_loss += dac_trainer.update_discriminator(
                    (expert_obs, expert_act),
                    (policy_obs, policy_act)
                )
            disc_loss /= 2.0

            # Log metrics every PRINT_INTERVAL timesteps
            if timestep % PRINT_INTERVAL == 0:
                if episodes_since_log > 0:
                    avg_reward = reward_sum_since_log / episodes_since_log
                else:
                    avg_reward = float("nan")

                writer.add_scalar("Metrics/MeanEpisodeReward", avg_reward, timestep)
                writer.add_scalar("Losses/ActorLoss", actor_loss, timestep)
                writer.add_scalar("Losses/CriticLoss", critic_loss, timestep)
                writer.add_scalar("Losses/DiscLoss", disc_loss, timestep)

                print(
                    f"Timesteps: {timestep}, "
                    f"Mean episode reward: {avg_reward:.2f}, "
                    f"Actor loss: {actor_loss:.3f}, "
                    f"Critic loss: {critic_loss:.3f}, "
                    f"Disc loss: {disc_loss:.3f}"
                )

                episodes_since_log = 0
                reward_sum_since_log = 0.0

    print("Training complete.")
    model_save_path = os.path.join(MODELS_DIR, MODEL_NAME)
    torch.save(actor.state_dict(), model_save_path + ".pt")
    print(f"DAC actor saved at {model_save_path}.pt")

    env.close()
    writer.close()


if __name__ == "__main__":
    print("Usage example:")
    print("python train_dac.py --env halfcheetah --timesteps 1000000 --seed 42")
    main()
