--env: HalfCheetah-v4
--demo_path: ./data/demonstrations/100/halfcheetah_demonstrations_100.npy
--episodes: 1000
--seed: 44
--max_steps: 1000
--batch_size: 256
--update_every: 10
--demo_episodes: 100

# --- Actor Network (Gaussian Policy) ---
class Actor(nn.Module):
    def __init__(self, state_dim, action_dim, hidden_dim=256, log_std_min=-20, log_std_max=2):
        super(Actor, self).__init__()
        self.log_std_min = log_std_min
        self.log_std_max = log_std_max
        self.net = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
        )
        self.mean_linear = nn.Linear(hidden_dim, action_dim)
        self.log_std_linear = nn.Linear(hidden_dim, action_dim)
        
    def forward(self, state):
        x = self.net(state)
        mean = self.mean_linear(x)
        log_std = self.log_std_linear(x)
        log_std = torch.clamp(log_std, self.log_std_min, self.log_std_max)
        return mean, log_std
...

# --- Critic Network (Double Q Network) ---
class Critic(nn.Module):
    def __init__(self, state_dim, action_dim, hidden_dim=256):
        super(Critic, self).__init__()
        # First Q network
        self.q1_net = nn.Sequential(
            nn.Linear(state_dim + action_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)
        )
        # Second Q network
        self.q2_net = nn.Sequential(
            nn.Linear(state_dim + action_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)
        )
    
    def forward(self, state, action):
        xu = torch.cat([state, action], dim=1)
        q1 = self.q1_net(xu)
        q2 = self.q2_net(xu)
        return q1, q2

...

# --- SQIL Agent based on SAC ---
class SQILAgent:
    def __init__(self, state_dim, action_dim, action_range=1.0, 
                 actor_hidden=256, critic_hidden=256, 
                 actor_lr=3e-4, critic_lr=3e-4, alpha_lr=1e-4,
                 gamma=0.99, tau=0.005, target_entropy=None,
                 demo_buffer_capacity=100000, agent_buffer_capacity=100000,
                 batch_size=256):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.action_range = action_range
        self.gamma = gamma
        self.tau = tau
        self.batch_size = batch_size
        
        # Actor network and its optimizer
        self.actor = Actor(state_dim, action_dim, hidden_dim=actor_hidden).to(device)
        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=actor_lr)
        
        # Critic network and its optimizer
        self.critic = Critic(state_dim, action_dim, hidden_dim=critic_hidden).to(device)
        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=critic_lr)
        
        # Target critic network
        self.critic_target = Critic(state_dim, action_dim, hidden_dim=critic_hidden).to(device)
        self.critic_target.load_state_dict(self.critic.state_dict())
        
        # Automatic entropy coefficient (alpha)
        if target_entropy is None:
            self.target_entropy = -action_dim  # Suggested value in SAC
        else:
            self.target_entropy = target_entropy
        self.log_alpha = torch.tensor(0.0, requires_grad=True, device=device)
        self.alpha_optimizer = optim.Adam([self.log_alpha], lr=alpha_lr)
        
        # Replay buffers for demonstrations and agent transitions
        self.demo_buffer = ReplayBuffer(demo_buffer_capacity)
        self.agent_buffer = ReplayBuffer(agent_buffer_capacity)
...

