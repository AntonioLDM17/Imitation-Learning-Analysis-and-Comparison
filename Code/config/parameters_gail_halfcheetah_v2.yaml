env: halfcheetah
timesteps: 2000000
seed: 44
demo_episodes: 100
learner:
  algorithm: TRPO
  policy: MlpPolicy
reward_net:
  type: BasicRewardNet
  hid_sizes:
  - 256
  - 256
  - 128
  normalize_input_layer: RunningNorm
gail_trainer:
  demo_batch_size: 2048
  gen_replay_buffer_capacity: 2048
  n_disc_updates_per_round: 16
  disc_opt_kwargs:
    lr: 0.0003
  allow_variable_horizon: true
architecture:
  policy_network:
    type: MLP
    hidden_layers:
    - 64
    - 64
    activation: Tanh
    head: "Gaussian (μ, σ) with tanh squash"
  value_network:
    shared_with_policy: false
    hidden_layers:
    - 64
    - 64
    activation: Tanh
  discriminator:
    type: BasicRewardNet
    hidden_layers:
    - 256
    - 256
    - 128
    activation: ReLU
    output: Scalar
    normalize_input_layer: RunningNorm
    input: concat(s, a, s_next)
training:
  method: Generative Adversarial Imitation Learning (GAIL)
  rl_algorithm: TRPO
  trpo_hyperparams:
    step_size: default
    gae_lambda: default
    gamma: default
  discriminator:
    demo_batch_size: 2048
    gen_replay_buffer_capacity: 2048
    n_disc_updates_per_round: 16
    learning_rate: 0.0003
  total_timesteps: 2000000
