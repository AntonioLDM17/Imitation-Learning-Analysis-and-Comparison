bc:
  env: halfcheetah
  timesteps: 2000000
  seed: 44
  demo_episodes: 100
  architecture:
    policy_network:
      type: MLP
      hidden_layers:
      - 64
      - 64
      activation: Tanh
      head: "Gaussian (μ, σ) with tanh squash"
    value_network:
      hidden_layers:
      - 64
      - 64
      activation: Tanh
      note: generated by SB3 MlpPolicy but not used in BC loss
  training:
    method: Behavioral Cloning (supervised)
    library: imitation.algorithms.bc + SB3 MlpPolicy
    optimizer: Adam
    learning_rate: 0.001
    batch_size: 64
    epochs: implicit (timesteps / demo_steps)
    loss: Negative Log-Likelihood
    total_timesteps: 2000000
bco:
  env: halfcheetah
  timesteps: 2000000
  seed: 44
  pre_interactions: 300000
  alpha: 0.0
  batch_size: 64
  demo_policy_epochs_map:
    5: 212
    10: 106
    20: 71
    50: 31
    100: 17
  inverse_dynamics:
    hidden_size: 256
    epochs: 10
    learning_rate: 0.001
  policy_network:
    hidden_size: 64
    epochs: 20
    learning_rate: 0.001
  architecture:
    policy_network:
      type: MLP
      hidden_layers:
      - 64
      - 64
      activation: ReLU
      output: action_dim
    inverse_dynamics:
      type: MLP
      hidden_layers:
      - 256
      - 256
      activation: ReLU
      layer_norm: true
      input: concat(s, s_next)
      output: action_dim
  training:
    method: Behavioral Cloning from Observation
    optimizer: Adam
    policy_lr: 0.001
    inverse_dynamics_lr: 0.001
    batch_size: 64
    loss:
      discrete: CrossEntropy
      continuous: MSE
    pre_interactions: 300000
    alpha: 0.0
    num_iterations: 0
gail:
  env: halfcheetah
  timesteps: 2000000
  seed: 44
  demo_episodes: 100
  learner:
    algorithm: TRPO
    policy: MlpPolicy
  reward_net:
    type: BasicRewardNet
    hid_sizes:
    - 256
    - 256
    - 128
    normalize_input_layer: RunningNorm
  gail_trainer:
    demo_batch_size: 2048
    gen_replay_buffer_capacity: 2048
    n_disc_updates_per_round: 16
    disc_opt_kwargs:
      lr: 0.0003
    allow_variable_horizon: true
  architecture:
    policy_network:
      type: MLP
      hidden_layers:
      - 64
      - 64
      activation: Tanh
      head: "Gaussian (μ, σ) with tanh squash"
    value_network:
      shared_with_policy: false
      hidden_layers:
      - 64
      - 64
      activation: Tanh
    discriminator:
      type: BasicRewardNet
      hidden_layers:
      - 256
      - 256
      - 128
      activation: ReLU
      output: Scalar
      normalize_input_layer: RunningNorm
      input: concat(s, a, s_next)
  training:
    method: Generative Adversarial Imitation Learning (GAIL)
    rl_algorithm: TRPO
    trpo_hyperparams:
      step_size: default
      gae_lambda: default
      gamma: default
    discriminator:
      demo_batch_size: 2048
      gen_replay_buffer_capacity: 2048
      n_disc_updates_per_round: 16
      learning_rate: 0.0003
    total_timesteps: 2000000
gaifo:
  env: halfcheetah
  timesteps: 2000000
  seed: 44
  demo_episodes: 100
  rollout_length: 2048
  learner:
    algorithm: TRPO
    policy: MlpPolicy
  discriminator:
    hidden_dim: 256
    batch_size: 512
    epochs: 20
    learning_rate: 5.4868671601784924e-05
    betas:
    - 0.9
    - 0.999
    lambda_gp: 5.165201675071828
  architecture:
    policy_network:
      type: MLP
      hidden_layers:
      - 64
      - 64
      activation: Tanh
      head: "Gaussian (μ, σ) with tanh squash"
    value_network:
      shared_with_policy: false
      hidden_layers:
      - 64
      - 64
      activation: Tanh
    discriminator:
      type: GAIfODiscriminator
      input: concat(s, s_next)
      hidden_layers:
      - 256
      - 256
      activation: ReLU
      output: Sigmoid
      gradient_penalty_lambda: 5.165201675071828
  training:
    method: Generative Adversarial Imitation from Observation (GAIfO)
    rl_algorithm: TRPO
    trpo_hyperparams:
      cg_max_steps: default
      gae_lambda: default
      gamma: default
      step_size: default
    discriminator:
      batch_size: 512
      epochs: 20
      learning_rate: 5.4868671601784924e-05
      betas:
      - 0.9
      - 0.999
      lambda_gp: 5.165201675071828
    rollout_length: 2048
    total_timesteps: 2000000
airl:
  env: halfcheetah
  timesteps: 2000000
  seed: 44
  demo_batch_size: 2048
  demo_episodes: 100
  learner:
    algorithm: PPO
    policy: MlpPolicy
    batch_size: 128
    ent_coef: 0.01
    learning_rate: 0.001
    gamma: 0.95
    clip_range: 0.1
    vf_coef: 0.1
    n_epochs: 5
  reward_net:
    type: BasicShapedRewardNet
    reward_hid_sizes:
    - 128
    - 128
    potential_hid_sizes:
    - 128
    - 128
    - 64
    use_state: true
    use_action: true
    use_next_state: true
    use_done: false
    discount_factor: 0.99
    normalize_input_layer: RunningNorm
  airl_trainer:
    gen_replay_buffer_capacity: 1024
    n_disc_updates_per_round: 24
    allow_variable_horizon: true
  architecture:
    policy_network:
      type: MLP
      hidden_layers:
      - 64
      - 64
      activation: ReLU
      head: "Gaussian (μ, σ) with tanh squash"
    value_network:
      shared_with_policy: false
      hidden_layers:
      - 64
      - 64
      activation: ReLU
    reward_network:
      type: BasicShapedRewardNet
      reward_hid_sizes:
      - 128
      - 128
      potential_hid_sizes:
      - 128
      - 128
      - 64
      activation: ReLU
      normalize_input_layer: RunningNorm
      use_state: true
      use_action: true
      use_next_state: true
      use_done: false
  training:
    method: Adversarial Imitation Learning - AIRL
    rl_algorithm: PPO
    ppo_hyperparams:
      batch_size: 128
      learning_rate: 0.001
      ent_coef: 0.01
      gamma: 0.95
      clip_range: 0.1
      vf_coef: 0.1
      n_epochs: 5
    discriminator:
      demo_batch_size: 2048
      gen_replay_buffer_capacity: 1024
      n_disc_updates_per_round: 24
    total_timesteps: 2000000
sqil:
  env: HalfCheetah-v4
  demo_path: ./data/demonstrations/100/halfcheetah_demonstrations_100.npy
  seed: 44
  episodes: 2000
  max_steps: 1000
  update_every: 10
  demo_episodes: 100
  agent:
    action_range: 1.0
    actor_hidden: 256
    critic_hidden: 256
    actor_lr: 0.0003
    critic_lr: 0.0003
    alpha_lr: 0.0001
    gamma: 0.99
    tau: 0.005
    target_entropy: null
    demo_buffer_capacity: 100000
    agent_buffer_capacity: 100000
    batch_size: 256
  actor:
    hidden_dim: 256
    log_std_min: -20
    log_std_max: 2
  critic:
    hidden_dim: 256
  architecture:
    actor_network:
      type: MLP
      hidden_layers:
      - 256
      - 256
      activation: ReLU
      head: "Gaussian (μ, σ) with tanh squash"
      log_std_range:
      - -20
      - 2
    critic_network:
      type: DoubleQ
      hidden_layers:
      - 256
      - 256
      activation: ReLU
      output: scalar Q-value
      note: Two independent Q networks (soft double Q-learning)
  training:
    method: Soft Q Imitation Learning (SQIL)
    rl_algorithm: SAC-like off policy
    hyperparams:
      actor_lr: 0.0003
      critic_lr: 0.0003
      alpha_lr: 0.0001
      gamma: 0.99
      tau: 0.005
      batch_size: 256
      target_entropy: null
      update_every: 10
    buffers:
      demo_buffer_capacity: 100000
      agent_buffer_capacity: 100000
    reward_scheme: Demonstrations rewarded with 1, agent data with 0
    total_episodes: 2000
    max_steps_per_episode: 1000
